\section{Related Work}
\label{sec:relatedwork}

Real-time distributed collaborative editors consider multiple members, each
hosting a copy of a shared sequence of characters. Each member's update
immediately apply to its local copy. Then, the operation is broadcast to all
other members where it is
re-executed~\cite{saito2005optimistic}. \TODO{Correctness} requires that all
members eventually converge to an identical state, i.e., when the system is
idle, all copies become similar~\cite{bailis2013eventual}. \TODO{Correctness}
also requires ensuring intention preservation, i.e., effects observed at
generation time must be re-observed at re-execution time regardless of
concurrent operations~\cite{sun1998achieving}. \TODO{Correctness} finally
requires ensuring causality, i.e., the re-execution order of operations follows
the \emph{happen before} relationship~\cite{lamport1978time} stated at
generation.

Several complexities characterize decentralized editors:
\begin{itemize}
\item Generation time: complexity to generate locally an operation.
\item Integration time: complexity to execute remotely an operation.
\item Space complexity: complexity to store a local copy of the shared sequence,
  including state descriptors, logs, etc.
\item Communication complexity: complexity of messages transiting the network.
\end{itemize}
Solving scalability issues requires finding a balanced trade-off between
communication, space and time complexities.  Among others, the communication
complexity is the most discriminant. First, it requires at least a
multiplicative logarithmic factor compared to the editing session size. Second,
messages contain metadata necessary to maintain consistency. To scale, the space
complexity of these metadata must be sublinear.

\begin{asparadesc}
\item [Operational transformation] (OT) allows building both centralized and
  decentralized real-time editors. At generation, the processing time of
  operations is constant. At integration, OT transforms the received operations
  against concurrent ones which were generated on the same state. An integration
  algorithm (e.g. COT~\cite{sun2009contextbased}, SOCT2~\cite{vidot200copies})
  along with transformation functions (ensuring transformation properties)
  guarantee a consistent model. The integration time depends of concurrency and
  differs among the algorithms.  For instance, COT's integration time complexity
  is exponential. It can reduce it to linear at the expense of space
  complexity. SOCT2's integration time complexity is quadratic.  Whatever the
  trade-off, OT's integration algorithms rely on concurrency detection which
  costs, at least, a state vector~\cite{charronbost1991concerning} the size of
  which grows linearly compared to the number of members who ever participated
  in the authoring. Since each message embeds such vector, and excepting humble
  confined environment (e.g. local area networks), decentralized OT approaches
  are not practicable. As for space complexity, OT approaches require an
  historic of operations, each operation being linked to their state
  vector. Such monotonically growing historic can be cut at the price of
  consensus which, once again, does not scale.

\item [Conflict-free replicated data types] (CRDTs) for sequences constitute the
  latter approaches which solve the concurrent cases by providing commutative
  and idempotent operations. Commutativity of operations ensures the correctness
  of the system. These approaches provide commutativity by associating a unique
  and immutable identifier to each element. Defining a total order among the
  identifiers allows retrieving the sequence. Compared to OT, CRDT approaches
  will increase the generation time complexity but do not require to detect
  concurrency at integration time. Hence, CRDTs can achieve logarithmic
  integration time that does not depend of concurrency. Moreover, as an
  operation is generated once and re-executed many times, CRDT provides an
  interesting trade-off. On space complexity, each element of a shared document
  will be decorated with a CRDT identifier. On the other hand, no log is
  required. Finally, on communication complexity, as CRDT do not require to
  detect concurrency, only the identifier of the element is piggy-backed. The
  size of this identifier determine the communication complexity.

  There exist two class of CRDTs for sequences exposing different
  trade-off.
\item [Tombstone-based] CRDTs such as WOOT\cite{oster2006data}
  associate a constant size identifier to each element but whose
  removals of elements only hide them to the users. Therefore, removed
  elements keep consumming space and the document will grow
  infinitely. Removing destroyed elements requires to run a costly
  consensus algorithm (\emph{does everyone see the removal and agree
    on definitely throw out the element}) which is prohibitevely
  costly and does not scale in number of users, especially in network
  subject to churn (where members join and leave the network
  frequently and freely).

\item [Variable-size identifiers] CRDTs whose removals truely destroy
  the targeted elements but whose identifiers are allocated with
  different size at generation. In these approaches, the allocation
  function is crucial to maintain identifiers under acceptable
  boundaries. Unfortunately, they depend of the insert position of
  elements. For instance, writing the sequence QWERTY left-to-right
  allocates the identifiers [1] to Q, [2] to W, [3] to E, \ldots, [6]
  to Y. But with an identical strategy, writing the same sequence
  right-to-left allocates the identifiers [1] to Y, [1.1] to T,
  [1.1.1] to R, \ldots As we observe, depending on the editing
  behaviour, the identifiers can grow quickly.  If the worst case
  happens, then a costly distributed consensus is required to
  rebalance identifiers as in~\cite{zawirski2011asynchronous}.

Finally, the LSEQ algorithm [11] aims to avoid consensus by bounding
variable- size identifiers to a space complexity sublinear to the size
of shared document. It conjectured that identifiers can be bounded to
the $log(s)^2$ where s is the size of shared document. In this paper,
we demonstrate in which conditions  this upper-bound can be proved.

\end{asparadesc}

Table~\ref{table:complexities} shows the upper-bound ($\mathcal{O}$)
of complexities in space, time, and communication of decentralized
approaches. In this table, we can see that both operationnal
transformations approaches, namely SOCT2/TTF and COT-DO, do not scale
in communication since they send messages that grows linearly compared
to the number of writers. Conversely, the representatives of
tombstone-based CRDTs (WOOT, WOOT, WOOTH, RGA, SW, PPS, DiCE)
communication complexity only depends of the dissemination protocol
(in logarithm of the number of replicas), however their space and time
complexities include the removals. In other terms, these approaches
are monotonically growing structure, hence, they do not scale in
number of operations performed on the document. The variable-size
identifiers CRDTs comprise Treedoc, Logoot, and \LSEQ. Their
complexities depend of insert positions (or editing behaviour) of the
elements. Treedoc, as a hybrid solution, uses tombstones and suffers
of the aforementionned issue. Like Logoot, it mainly targets monotonic
editing at the end of the sequence. Unfortunately, when the editing
behaviour does not comply with this assumption, the space complexity
grows quadratically. \LSEQ provides a sublinear upper-bound on its
space complexity. Thus, it scales in document size, and
communications. Yet, it requires a local causality tracking mechanism
increasing linearly compared to the number of
writers. \TODO{cf. proposal?}.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: 
