\section{Related Work}
\label{sec:relatedwork}

Real-time distributed collaborative editors consider multiple members, each
hosting a copy of a shared sequence of characters. Each member's update
immediately apply to its local copy. Then, the operation is broadcast to all
other members where it is re-executed~\cite{saito2005optimistic}. Consistency
requires that all members eventually converge to an identical state, i.e., when
the system is idle, all copies become
similar~\cite{bailis2013eventual}. Consistency also requires ensuring intention
preservation, i.e., effects observed at generation time must be re-observed at
re-execution time regardless of concurrent
operations~\cite{sun1998achieving}. Consistency finally requires ensuring
causality, i.e., the re-execution order of operations follows the \emph{happen
  before} relationship~\cite{lamport1978time} stated at generation.

Several complexities characterize decentralized editors:
\begin{itemize}
\item Generation time: complexity to generate locally an operation.
\item Integration time: complexity to execute remotely an operation.
\item Space complexity: complexity to store a local copy of the shared sequence,
  including state descriptors, logs, etc.
\item Communication complexity: complexity of messages transiting the network.
\end{itemize}
Solving scalability issues requires finding a balanced trade-off between
communication, space and time complexities.  Among others, the communication
complexity is the most discriminant. First, it requires at least a
multiplicative logarithmic factor compared to the editing session size
$R$. Second, messages contain metadata necessary to maintain consistency. To
scale, the space complexity of these metadata must be sublinear.

\begin{asparadesc}
\item [Operational transformation] (OT) allows building both centralized and
  decentralized real-time editors. At generation, the processing time of
  operations is constant. At integration, OT transforms the received operations
  against concurrent ones which were generated on the same state. An integration
  algorithm (e.g. COT~\cite{sun2009contextbased}, SOCT2~\cite{vidot2000copies})
  along with transformation functions (ensuring transformation properties)
  guarantee a consistent model. The integration time depends of concurrency and
  differs among the algorithms.  For instance, COT's integration time complexity
  is exponential. It can reduce it to linear at the expense of space
  complexity. SOCT2's integration time complexity is quadratic.  Whatever the
  trade-off, OT's integration algorithms rely on concurrency detection which
  costs, at least, a state vector~\cite{charronbost1991concerning} the size of
  which grows linearly compared to the number of members $W$ who ever
  participated in the authoring. Since each message embeds such vector, and
  excepting humble confined environment (e.g. local area networks),
  decentralized OT approaches are not practicable. As for space complexity, OT
  approaches require an historic of operations $H$, each operation being linked
  to their state vector, hence $\mathcal{O}(W.H)$. Such monotonically
  growing historic can be cut at the price of consensus which, once again, does
  not scale.

\item [Conflict-free replicated data types] (CRDTs) for sequences constitute the
  latter approaches which solve concurrent cases by providing commutative and
  idempotent operations. As such, and compared to OT, the causality tracking
  cost is drastically reduced since it only requires tracking semantically
  related operations. For instance, the removal of a particular element must
  follow its insertion. The commutative property of operations ensures the
  consistency of the system. CRDTs provide commutativity by associating a unique
  and immutable identifier to each element. Defining a total order among the
  identifiers allows retrieving the sequence of characters. 

  CRDTs propose interesting trade-offs since they can balance complexities
  depending on the type of structure that represents the document.  In
  particular, increasing the generation time of operations to decrease the
  integration time is profitable since an operation generated once is
  re-executed many times. Nevertheless, the identifiers consume space which, in
  turns, impacts the communication complexity.

% WOOT, WOOT, WOOTH, RGA, SW, PPS, DiCE
\item [Tombstone-based] CRDTs such as WOOT~\cite{oster2006data} associate a
  constant size identifier $\mathcal{O}(1)$ to each element but whose removals
  of elements only hide them to the users. Therefore, removed elements keep
  consuming space leading to a monotonically growing document, hence
  $\mathcal{O}(H)$. For instance, Wikipedia pages subject to vandalism could
  consume a lot of memory while appearing empty. Destroying removed elements
  requires running a costly consensus algorithm (\emph{does everyone see the
    removal and agree on definitely throw out the element}) which is
  prohibitively costly and does not scale in number of members, especially in
  network subject to churn (where they join and leave the network frequently and
  freely). Since the structure contains the full history of the document, such
  approach do not require further information to track causally related
  operations.

\item [Variable-size identifiers] CRDTs truly destroy elements targeted by
  removals. However, they allocate identifiers the size of which is determined
  at generation. The allocation function becomes crucial to maintain identifiers
  under acceptable boundaries. Unfortunately, they depend of the insert position
  of elements. For instance, writing the sequence QWERTY left-to-right allocates
  the identifiers [1] to Q, [2] to W, [3] to E, \ldots, [6] to Y. But with an
  identical strategy, writing the same sequence right-to-left allocates the
  identifiers [1] to Y, [1.1] to T, [1.1.1] to R, \ldots We observe a quick
  growth of identifiers depending on the editing behavior. In both cases, the
  growth is linear compared to the number of insertions in the sequence, i.e.,
  $\mathcal{O}(I)$. If the structure stores each identifier in a flat array, it
  grants fast access at the price of high space complexity
  ($\mathcal{O}(I^2)$). If it factorizes common parts of identifiers as a tree
  structure, it achieves better space complexity. Yet, in the worst case, a
  costly distributed consensus is still required to balance the
  structure~\cite{zawirski2011asynchronous}. Since removals truly erase
  information from the structure, these approaches require a local state vector
  compacting their history, hence an additional $\mathcal{O}(W)$ on
  space complexity. It worth noting that, contrarily to OT, the vector is kept
  local.

  Algorithm \LSEQ~\cite{nedelec2013lseq} aims to avoid such consensus by
  sublinearly upper-bounding the space complexity of variable-size
  identifiers. It conjectured a polylogarithmic progression of the identifiers
  size $\mathcal{O}(log(I)^2)$. In this paper, we demonstrate this upper-bound
  and we state the conditions under which it applies.

%%\item [Table~\ref{table:complexities} summarizes] the boundaries of complexities
%%  in communication and space of decentralized approaches. It highlights the
%%  bottleneck implied by each class of approaches.
\end{asparadesc}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: 
