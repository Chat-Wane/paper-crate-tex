\section{Related Work}
\label{sec:relatedwork}

\begin{table*}
  \centering
  \input{./input/complexitiestable.tex}
  \caption{\label{table:complexities}
    Communication and space complexities of decentralized approaches.}
\end{table*}

Real-time distributed collaborative editors consider multiple members, each
hosting a copy of a shared sequence of characters. Each member's update
immediately apply to its local copy. Then, the operation is broadcast to all
other members where it is re-executed~\cite{saito2005optimistic}. Correctness
requires that all members eventually converge to an identical state, i.e., when
the system is idle, all copies become
similar~\cite{bailis2013eventual}. Correctness also requires to ensure intention
preservation, i.e., effects observed at generation time must be re-observed at
re-execution time regardless of concurrent
operations~\cite{sun1998achieving}. \TODO{Causality?}

Several complexities characterize decentralized editors:
\begin{itemize}
\item Generation time: complexity to generate locally an operation.
\item Integration time: complexity to execute remotely an operation.
\item Space complexity: complexity to store a local copy of the shared sequence,
  including state descriptors, logs, etc.
\item Communication complexity: complexity of messages transiting the network.
\end{itemize}
Solving scalability issues requires finding a balanced trade-off between
communication, space and time complexities.  Among others, the communication
complexity is the most discriminant. First, it requires at least a
multiplicative logarithmic factor compared to the editing session size. Second,
messages contain metadata necessary to maintain consistency. To scale, the space
complexity of these metadata must be sublinear.

\begin{asparadesc}
\item [Operational transformation (OT)] allows to build centralized or
  decentralized real-time editors. In OT, operations are generated in
  constant time. Next, an operation has to be transformed according to
  concurrent operations defined on the same state. Consistency is
  obtained thanks to an OT integration algorithm such as COT or SOCT2
  and properties on transformation functions. The Integration time
  complexity depends of concurrency and can be different according to
  different trade-off. In COT~\cite{sun2009contextbased}, time
  complexity of integration is exponential. It can be reduced to
  linear at the cost of high space complexity. In
  SOCT2~\cite{vidot2000copies}, it is quadratic. Whatever the trade-off chosen
  in the integration algorithm, OT has to transform incoming
  operations according to concurrent operations and a state vector is
  the minimal structure that allows to detect
  concurrency~\cite{charronbost1991concerning}. Hence, OT requires to
  piggy back to operations sent on the network a structure at least
  linear to the number of participants. On space complexity, OT keeps
  the document as a non-collaborative editor can do, plus a log of
  operations. In the worst case i.e. a site is not responding, the
  size of the this log includes all the operations produced from the
  beginning of editing session, where each operation is decorated with
  state vector proportional to number of participants. If average
  case, a garbage collecting algorithm can drop operations integrated
  by all participants. So, the slower participant will determine the
  size of log hosted by all participant. OT is a powerfull approach
  for small groups, performances degrade when the group is growing.

\item [Conflict-free replicated data types] (CRDTs) for sequences
  constitute the latter approaches which solve the concurrent cases by
  providing commutative and idempotent operations. Commutativity of
  operations ensures the correctness of the system. These approaches
  provide commutativity by associating a unique and immutable
  identifier to each element. Defining a total order among the
  identifiers allows retrieving the sequence. Compared to OT, CRDT
  approaches will increase the generation time complexity but do not
  require to detect concurrency at integration time. Hence, CRDTs can
  achieve logarithmic integration time that does not depend of
  concurrency. Moreover, as an operation is generated once and
  re-executed many times, CRDT provides an interesting trade-off. On
  space complexity, each element of a shared document will be
  decorated with a CRDT identifier. On the other hand, no log is
  required. Finally, on communication complexity, as CRDT do not require to
  detect concurrency, only the identifier of the element is
  piggy-backed. The size of this identifier determine the
  communication complexity.

  There exist two class of CRDTs for sequences exposing different
  trade-off.
\item [Tombstone-based] CRDTs such as WOOT\cite{oster2006data}
  associate a constant size identifier to each element but whose
  removals of elements only hide them to the users. Therefore, removed
  elements keep consumming space and the document will grow
  infinitely. Removing destroyed elements requires to run a costly
  consensus algorithm (\emph{does everyone see the removal and agree
    on definitely throw out the element}) which is prohibitevely
  costly and does not scale in number of users, especially in network
  subject to churn (where members join and leave the network
  frequently and freely).

\item [Variable-size identifiers] CRDTs whose removals truely destroy
  the targeted elements but whose identifiers are allocated with
  different size at generation. In these approaches, the allocation
  function is crucial to maintain identifiers under acceptable
  boundaries. Unfortunately, they depend of the insert position of
  elements. For instance, writing the sequence QWERTY left-to-right
  allocates the identifiers [1] to Q, [2] to W, [3] to E, \ldots, [6]
  to Y. But with an identical strategy, writing the same sequence
  right-to-left allocates the identifiers [1] to Y, [1.1] to T,
  [1.1.1] to R, \ldots As we observe, depending on the editing
  behaviour, the identifiers can grow quickly.  If the worst case
  happens, then a costly distributed consensus is required to
  rebalance identifiers as in~\cite{zawirskiasynchronous}.

Finally, the LSEQ algorithm [11] aims to avoid consensus by bounding
variable- size identifiers to a space complexity sublinear to the size
of shared document. It conjectured that identifiers can be bounded to
the $log(s)^2$ where s is the size of shared document. In this paper,
we demonstrate in which conditions  this upper-bound can be proved.

\end{asparadesc}

Table~\ref{table:complexities} shows the upper-bound ($\mathcal{O}$)
of complexities in space, time, and communication of decentralized
approaches. In this table, we can see that both operationnal
transformations approaches, namely SOCT2/TTF and COT-DO, do not scale
in communication since they send messages that grows linearly compared
to the number of writers. Conversely, the representatives of
tombstone-based CRDTs (WOOT, WOOT, WOOTH, RGA, SW, PPS, DiCE)
communication complexity only depends of the dissemination protocol
(in logarithm of the number of replicas), however their space and time
complexities include the removals. In other terms, these approaches
are monotonically growing structure, hence, they do not scale in
number of operations performed on the document. The variable-size
identifiers CRDTs comprise Treedoc, Logoot, and \LSEQ. Their
complexities depend of insert positions (or editing behaviour) of the
elements. Treedoc, as a hybrid solution, uses tombstones and suffers
of the aforementionned issue. Like Logoot, it mainly targets monotonic
editing at the end of the sequence. Unfortunately, when the editing
behaviour does not comply with this assumption, the space complexity
grows quadratically. \LSEQ provides a sublinear upper-bound on its
space complexity. Thus, it scales in document size, and
communications. Yet, it requires a local causality tracking mechanism
increasing linearly compared to the number of
writers. \TODO{cf. proposal?}.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: 
