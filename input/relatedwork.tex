\section{Related Work}
\label{sec:relatedwork}

Real-time distributed collaborative editors consider multiple members, each
hosting a copy of a shared sequence of characters. Each member's update
immediately apply to its local copy. Then, the operation is broadcast to all
other members where it is re-executed~\cite{saito2005optimistic}. Consistency
requires that all members eventually converge to an identical state, i.e., when
the system is idle, all copies become
similar~\cite{bailis2013eventual}. Consistency also requires ensuring intention
preservation, i.e., effects observed at generation time must be re-observed at
re-execution time regardless of concurrent
operations~\cite{sun1998achieving}. Consistency finally requires ensuring
causality, i.e., the re-execution order of operations follows the \emph{happen
  before} relationship~\cite{lamport1978time} stated at generation.

Several complexities characterize decentralized editors:
\begin{itemize}
\item Generation time: complexity to generate locally an operation.
\item Integration time: complexity to execute remotely an operation.
\item Space complexity: complexity to store a local copy of the shared sequence,
  including state descriptors, logs, etc.
\item Communication complexity: complexity of messages transiting the network.
\end{itemize}
Solving scalability issues requires finding a balanced trade-off between
communication, space and time complexities.  Among others, the communication
complexity is the most discriminant. First, it requires at least a
multiplicative logarithmic factor compared to the editing session size. Second,
messages contain metadata necessary to maintain consistency. To scale, the space
complexity of these metadata must be sublinear.

\begin{asparadesc}
\item [Operational transformation] (OT) allows building both centralized and
  decentralized real-time editors. At generation, the processing time of
  operations is constant. At integration, OT transforms the received operations
  against concurrent ones which were generated on the same state. An integration
  algorithm (e.g. COT~\cite{sun2009contextbased}, SOCT2~\cite{vidot2000copies})
  along with transformation functions (ensuring transformation properties)
  guarantee a consistent model. The integration time depends of concurrency and
  differs among the algorithms.  For instance, COT's integration time complexity
  is exponential. It can reduce it to linear at the expense of space
  complexity. SOCT2's integration time complexity is quadratic.  Whatever the
  trade-off, OT's integration algorithms rely on concurrency detection which
  costs, at least, a state vector~\cite{charronbost1991concerning} the size of
  which grows linearly compared to the number of members who ever participated
  in the authoring. \TODO{Since each message embeds such vector, and excepting
    humble confined environment (e.g. local area networks), decentralized OT
    approaches are not practicable.} As for space complexity, OT approaches
  require an historic of operations, each operation being linked to their state
  vector. Such monotonically growing historic can be cut at the price of
  consensus which, once again, does not scale.

\item [Conflict-free replicated data types] (CRDTs) for sequences constitute the
  latter approaches which solve concurrent cases by providing commutative and
  idempotent operations. As such, and compared to OT, the causality tracking
  cost is drastically reduced since it only requires tracking semantically
  related operations. For instance, the removal of a particular element must
  follow its insertion. The commutative property of operations ensures the
  consistency of the system. CRDTs provide commutativity by associating a unique
  and immutable identifier to each element. Defining a total order among the
  identifiers allows retrieving the sequence of characters. 

  CRDTs propose interesting trade-offs since they can balance complexities
  depending on the type of structure that represents the document.  In
  particular, increasing the generation time of operations to decrease the
  integration time is profitable since an operation generated once is
  re-executed many times. Nevertheless, the identifiers consume space which, in
  turns, impacts the communication complexity.

\item [Tombstone-based] CRDTs such as WOOT~\cite{oster2006data} associate a
  constant size identifier to each element but whose removals of elements only
  hide them to the users. Therefore, removed elements keep consuming space,
  hence the infinitely growing document. For instance, Wikipedia pages subject
  to vandalism could consume a lot of memory while appearing empty. Removing
  destroyed elements requires running a costly consensus algorithm (\emph{does
    everyone see the removal and agree on definitely throw out the element})
  which is prohibitevely costly and does not scale in number of members,
  especially in network subject to churn (where they join and leave the network
  frequently and freely).

\item [Variable-size identifiers] CRDTs truly destroy elements targeted by
  removals. However, they allocate identifiers the size of which is determined
  at generation. The allocation function becomes crucial to maintain identifiers
  under acceptable boundaries. Unfortunately, they depend of the insert position
  of elements. For instance, writing the sequence QWERTY left-to-right allocates
  the identifiers [1] to Q, [2] to W, [3] to E, \ldots, [6] to Y. But with an
  identical strategy, writing the same sequence right-to-left allocates the
  identifiers [1] to Y, [1.1] to T, [1.1.1] to R, \ldots We observe a quick
  growth of identifiers depending on the editing behavior.  In the worst case, a
  costly distributed consensus is required to balance the underlying
  structure~\cite{zawirski2011asynchronous}.

  Algorithm \LSEQ~\cite{nedelec2013lseq} aims to avoid such consensus by
  sublinearly upper-bounding the space complexity of variable-size
  identifiers. It conjectured an upper-bound in $\mathcal{O}(log(I)^2)$ where
  $I$ is the document size. In this paper, we demonstrate in \TODO{which
    conditions this upper-bound can be proved.}

\item [As summary,] Table~\ref{table:complexities} shows the upper-bound
  ($\mathcal{O}$) of complexities in space, time, and communication of
  decentralized approaches. In this table, we can see that both operationnal
  transformations approaches, namely SOCT2/TTF and COT-DO, do not scale in
  communication since they send messages that grows linearly compared to the
  number of writers. Conversely, the representatives of tombstone-based CRDTs
  (WOOT, WOOT, WOOTH, RGA, SW, PPS, DiCE) communication complexity only depends
  of the dissemination protocol (in logarithm of the number of replicas),
  however their space and time complexities include the removals. In other
  terms, these approaches are monotonically growing structure, hence, they do
  not scale in number of operations performed on the document. The variable-size
  identifiers CRDTs comprise Treedoc, Logoot, and \LSEQ. Their complexities
  depend of insert positions (or editing behaviour) of the elements. Treedoc, as
  a hybrid solution, uses tombstones and suffers of the aforementionned
  issue. Like Logoot, it mainly targets monotonic editing at the end of the
  sequence. Unfortunately, when the editing behaviour does not comply with this
  assumption, the space complexity grows quadratically. \LSEQ provides a
  sublinear upper-bound on its space complexity. Thus, it scales in document
  size, and communications. Yet, it requires a local causality tracking
  mechanism increasing linearly compared to the number of
  writers. \TODO{cf. proposal?}.
\end{asparadesc}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../paper"
%%% End: 
